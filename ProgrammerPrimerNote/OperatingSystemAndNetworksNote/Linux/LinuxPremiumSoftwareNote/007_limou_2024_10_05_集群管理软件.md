>   资料：
>
>   https://www.bilibili.com/video/BV1w4411y7Go/?vd_source=c92c89dbfcf9cc30c48086469621f35b 视频
>
>   https://minikube.sigs.k8s.io/docs/tutorials/ 教程
>
>   https://minikube.sigs.k8s.io/docs/handbook/ 手册

# 1.历史发展

集群管理（资源管理）=Apche MESOS（开源分布式管理框架，被推特使用）->Apche MESOS 平台管理 Kubernetes，可以结合使用->Docker Swarm（非常轻量，专为 docker 容器准备，但是依旧不如 k8s，功能对于企业过于局限，但是不是不好用，有一个人通过捐赠得到了一个上千万的集群，可以搜索一下）->Kubernetes（由谷歌内部的 borg 使用 go 翻写而来，本身也是轻量级，开源，弹性伸缩，自动负载均衡，支持 `IPVS`）。

待补充...

# 2.相关的环境配置和软件安装

您必须先下载 `Docker` 后再来 [根据官方文档下载一些必要工具](https://kubernetes.io/docs/tasks/tools/)，统一下载后我们再来进行研究。

```shell
# 下载 kubectl 工具
sudo apt-get install -y apt-transport-https ca-certificates curl gnupg

curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.31/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg

sudo chmod 644 /etc/apt/keyrings/kubernetes-apt-keyring.gpg

echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.31/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list

sudo chmod 644 /etc/apt/sources.list.d/kubernetes.list

sudo apt-get update

sudo apt-get install -y kubectl

```

```shell
# 下载 minikube 集群管理工具
curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube_latest_amd64.deb

sudo dpkg -i minikube_latest_amd64.deb

minikube version
minikube version: v1.34.0
commit: 210b148df93a80eb872ecbeb7e35281b3c582c61
```

```shell
# 下载 kind 集群管理工具
wget https://golang.org/dl/go1.20.linux-amd64.tar.gz

sudo tar -C /usr/local -xzf go1.20.linux-amd64.tar.gz

echo 'export PATH=$PATH:/usr/local/go/bin' >> ~/.bashrc # 看终端

source ~/.bashrc

go version

go install sigs.k8s.io/kind@v0.24.0

echo 'export PATH=$PATH:$(go env GOPATH)/bin' >> ~/.bashrc
source ~/.bashrc

kind --version
kind version 0.24.0

```

简单提及一下 `minilube` 和 `kind` 可以提供 `k8s` 集群环境（相当于集群脚手架），而 `kubectl` 负责操作和管理该 `k8s` 集群的资源。

>   补充：除此以外，还有一些别的集群脚手架，例如 `K3s、Microk8s`。

# 3.k8s 的基础使用

## 3.1.k8s 的前世今生

`k8s` 可以说是 `Borg` 的加强版本，并且使用 `Go` 进行重构而来。

![image-20241011235648406](./assets/image-20241011235648406.png)

上图就是 `Borg` 的设计架构图。`BorgMaster(主节点)` 通过 `scheduler(调度器)`，来进行请求分发（并且一般是有多个副本，并且最好是奇数节点，避免出现“平局”的现象），而工作的节点就是一系列的 `Borglet(工作节点)`。

而这里有三种访问 `BorgMaster(主节点)` 的主要方法：

1.   配置文件读写
2.   命令行工具读写
3.   浏览器读写

其中 `scheduler(调度器)` 不会直接访问 `Borglet(工作节点)`，而是和 `Paxos(键值对数据库)` 进行交互，`Borglet` 就会从这个 `Paxos(键值对数据库)` 取出交给自己的请求进行消费。

而 `k8s` 架构图和上述类似。

## 3.2.k8s 的组件框架

![image-20241012000737073](./assets/image-20241012000737073.png)

上图就是 `k8s` 的设计架构图。`scheduler(调度器)` 会把任务交给 `api server(接口服务)`，`api server(接口服务)` 再把请求写入到 `etcd` 中。`replication controller(控制器)` 则用来控制 `api server(接口服务)` 副本的数量（此时可以动态修改副本数量，并且最好也是设置奇数个）。

因此这里也有三种访问 `api server(接口服务)` 的主要方式：

-   `etcd`  分布式键值对存储系统读写
-   `kubectl` 专用命令行工具读写
-   浏览器读写

这里还可以给出关于 `etcd` 的架构图。

![image-20241012003048888](./assets/image-20241012003048888.png)

因此都是 `C/S` 服务开发，`Raft` 是分布式的选举算法，通过选举领导的方式来处理写入请求。而为了防止数据损坏，还配备了 `WAL` 这种日志记录机制，防止数据的意外丢失。`Entry` 记录日志中的单个条目，包含一个操作或状态变更的具体信息，代表一个对状态机的修改。`Snapshot` 是状态机在某一时刻的完整快照，包含当前所有数据的状态，可以在系统恢复时避免重放所有的 `Entries`。最后才实际存储在 `Store` 中进行持久化。

>   补充：`WAL(Write-Ahead Logging, 预写日志)`是一种用于数据持久化的日志记录机制。在写入数据库之前，首先将数据变更记录到日志中。这种机制确保了在系统崩溃或故障后，可以通过回放日志来恢复数据的完整性。
>
>   `WAL` 的主要优点包括：
>
>   1.  **数据安全性** 在数据写入数据库之前，先记录到日志中，即使发生故障，也能通过日志恢复数据
>   2.  **性能提升** 批量写入时，可以先将日志写入内存，减少磁盘 `I/O` 操作，提高性能
>   3.  **一致性** 通过日志，确保数据变更操作的顺序一致，维护数据的一致性

>   补充：现在 `Etcd` 转而使用 `v3` 以上的版本，前面的版本已经被抛弃（ `v2` 版本几乎只把数据存储在内存中，`v3` 版本增加了关于存储卷的持久化方案）...

上述的所有重要组件都可以看作是 `k8s` 的主节点，接下来我们来看我们的工作节点。一个工作节点就是一个 `Node`，多个工作节点加上主节点就是一个 `k8s` 集群。而 `Node` 内部包含 `Kubelet(CRI, 容器/运行时环境/接口, 会维护和 Docker 容器的交互和操作)` 、`Kube proxy(负责处理集群中 Pod 的网络流量, 支持 ipvs 以及多种代理模式, 默认的操作对象是 firewail, 也就是防火墙去实现 Pod 的映射, 设置 IPTables、IPVS 实现服务映射访问`、`Pods(每个 Pod 内部包含多个容器, 可以是 Docker 容器)`。

>   补充：这里的 `api server(接口服务)` 非常繁忙（控制器、调度器、命令行工具、浏览器、存储系统、容器运行时接口、网络代理都需要访问），因此每个组件也可以在本地生成一定的缓存降低 `api server(接口服务)` 的压力（这种缓存机制可以由多种方案解决，比如设置缓存有效期 `TTL`，版本号时间戳确认请求，服务器事件驱动机制...）。不过这么讲有些简单，后面还会细细讲解...

>   补充：`IPVS(IP Virtual Server)`是一个 `Linux` 内核模块，提供基于 `IP` 的负载均衡功能。它通过将多个后端服务器（如 `Pod`）绑定到一个虚拟 `IP` 地址，实现负载均衡。
>
>   主要特点包括：
>
>   1.  **高性能**：`IPVS` 在内核中实现，能够处理大量并发连接，提供更高的性能和更低的延迟。
>   2.  **多种负载均衡算法**：支持多种负载均衡算法（如轮询、最少连接、加权轮询等），可以根据不同的场景选择合适的算法。
>   3.  **集成支持**：`Kubernetes` 中的 `Kube Proxy` 可以配置为使用 `IPVS` 模式，以便更高效地管理服务流量。
>
>   通过这些功能，`IPVS` 可以在大规模的分布式系统中有效地管理流量，提高应用的可用性和可靠性。

主要组件如上，但是还有一些非常重要的插件也值得一提。

- `CoreDNS` 可以为集群中的服务（`SVC, service, 服务, 在这里也就是一个 Pod 公开为网络服务`）创建一个域名服务器 `IP` 对应的解析
- `Dashboard` 可以 `k8s` 提供关于 `B/S` 架构的支持
- `Ingress Controller` 可以实现七层代理（官方只实现了四层）
- `Fedetation` 可以提供跨集群中心多 `k8s` 统一管理功能
- `Prometheus` 可以 `k8s` 集群的日志统一分析平台

## 3.3.k8s 的资源清单

在 `Kubernetes` 中，**资源清单** 指的是用于描述 `k8s` 集群中各类资源的声明性配置文件，通常以 `YAML` 或 `JSON` 格式编写。资源清单定义了集群中工作负载（如何运行、如何进行网络配置、如何管理存储和资源调度等。常见的资源清单类型：

1. **Pod**：最小的可调度单位，包含一个或多个容器。
   
   ```yaml
   apiVersion: v1
   kind: Pod
   metadata:
     name: my-pod
   spec:
     containers:
       - name: my-container
         image: nginx
   ```
   
2. **Deployment**：管理 Pod 副本的控制器，提供声明式更新。
   ```yaml
   apiVersion: apps/v1
   kind: Deployment
   metadata:
     name: my-deployment
   spec:
     replicas: 3
     selector:
       matchLabels:
         app: my-app
     template:
       metadata:
         labels:
           app: my-app
       spec:
         containers:
           - name: my-container
             image: nginx
   ```

3. **Service**：将一组 Pod 公开为网络服务，允许其他服务或用户通过固定 IP 访问这些 Pod。
   ```yaml
   apiVersion: v1
   kind: Service
   metadata:
     name: my-service
   spec:
     selector:
       app: my-app
     ports:
       - protocol: TCP
         port: 80
         targetPort: 8080
   ```

4. **Ingress**：定义从外部访问集群内服务的路由规则（通常是 HTTP/HTTPS）。
   ```yaml
   apiVersion: networking.k8s.io/v1
   kind: Ingress
   metadata:
     name: my-ingress
   spec:
     rules:
       - host: my-app.example.com
         http:
           paths:
             - path: /
               pathType: Prefix
               backend:
                 service:
                   name: my-service
                   port:
                     number: 80
   ```

5. **ConfigMap**：用于存储应用的配置信息。
   ```yaml
   apiVersion: v1
   kind: ConfigMap
   metadata:
     name: my-config
   data:
     config.yaml: |
       setting1: "value1"
       setting2: "value2"
   ```

6. **Secret**：存储敏感信息，比如密码、密钥等。
   ```yaml
   apiVersion: v1
   kind: Secret
   metadata:
     name: my-secret
   data:
     password: bXktc2VjcmV0LXBhc3N3b3Jk
   ```

我们来一个一个介绍...

### 3.3.1.Pod

`Pod` 可以分为两种：

- 自主式 `Pod`，生命周期结束后没有处理策略
- 控制器 `Pod`，生命周期结束后存在处理策略

而更加准确来说，`Pod` 只要运行起来，一定有一个容器被运行起来，而如果后续 `Pod` 中加入了其他容器，就必须保证这些容器使用一些相同的资源。这种做法就需要用一个 `Pause` 父容器来做到，它就相当于其他容器的父进程，可以共享给多个容器自己管理的资源（一般共享网络、共享存储卷）。

而 `Pod` 的控制器则可以分为三种：

- **ReplicationController / ReplicaSet**
  - `ReplicationController` 是 `k8s` 早期版本使用的控制器，用于确保指定数量的 `Pod` 实例在集群中始终保持运行状态（多则删，少则增）
  - `ReplicaSet` 是 `ReplicationController` 的改进版，功能类似，但提供了更灵活的标签选择器（`selectors`）。它的作用也是维持某个指定数量的 `Pod` 副本（`replicas`）运行，即使某些 `Pod` 崩溃或删除，`ReplicaSet` 会创建新的 `Pod` 来确保副本数恒定（但是一般还是会要求使用二十一时四十五分`Reployment` 来自动管理，因为有可能 `ReplicaSet` 不支持 `rolling-updata, 滚动更新`）
- **Deployment**
  - `Deployment` 是一种更高级的控制器，用于管理无状态应用的部署。它通常用于管理应用的滚动更新、回滚、扩展和其他运维任务
  -  `Deployment` 会创建一个 `ReplicaSet`，再由 `ReplicaSet` 来管理 `Pod` 的副本，并由 `ReplicaSet` 提供更新和版本控制的功能（可以方便杀掉原本的 `Pod`，重新建立新标签版本的 `Pod`，如果此次滚动更新导致`bug`，也可以进行版本回滚）
-  **Horizontal Pod Autoscaler**
  - `HPA` 根据用户定义的指标（通常是指 `Pod` 的 `CPU` 使用率、内存使用率或其他自定义指标）来动态调整 `Pod` 的副本数
  - `HPA` 也是基于 `ReplicaSet` 的，如果发现 `Pod` 指标过大，就会制作 `Pod` 的副本降低容器指标，一旦利用率变过低就会回收副本，也就做到了所谓的水平拓展
- **StatefulSet**
  - 为了解决 `Docker` 这种无状态容器的一种控制器，上面提到的三种控制器都是为无状态而设计的，只有 `StatefulSet` 是为了有状态而设计的
  - 每个 `Pod` 在 `StatefulSet` 中都有一个唯一的、稳定的网络标识符（稳定网络标识），`Pod` 名称是由 `StatefulSet` 名称和一个序号（例如 `myapp-0`, `myapp-1`）组成，这使得每个 `Pod` 可以通过其稳定的 `DNS` 名称进行访问，不会因为`Pod` 被顶替就发生改变
  - 每个 `Pod` 都可以绑定一个持久卷（稳定持久化存储），即使在 `Pod` 重启或迁移的情况下，数据也能保持一致。持久卷可以为每个 `Pod` 提供独立的存储，确保应用的状态不会丢失
  - `StatefulSet` 确保 `Pods` 按顺序部署和终止（有序部署/有序拓展，有序收缩/有序删除），在扩展或缩减 `StatefulSet` 时，`Pods` 的创建和删除遵循特定顺序，这对于需要有序启动或关闭的应用（例如数据库）非常重要

> 补充：`Docker` 被称为“无状态”的主要原因是其设计理念和容器化的工作方式。尽管您可以将数据文件封装进 `Docker` 镜像中，但这并不改变 `Docker` 本身的无状态特性。
>
> 1. **容器的临时性** `Docker` 容器通常是短暂的，启动和停止相对容易。它们被设计为在需要时快速启动，而不是长期运行。因此，如果一个容器崩溃或被删除，任何未持久化的数据都将丢失。
> 2. 数据持久化 虽然可以将数据文件放入容器中，但如果不使用数据卷（Volumes），这些数据在容器删除后将丢失。Docker 通过引入数据卷来实现持久化存储，允许容器与宿主机的文件系统共享数据。数据卷提供了一种持久化的方式，但容器本身仍然是无状态的。
> 3. **不依赖于容器的状态** 在微服务架构中，无状态服务不依赖于任何特定的请求上下文或会话信息。这使得服务更易于扩展和管理。无状态设计的服务可以在多个实例间负载均衡，任意实例的崩溃或重启不会影响整体服务。
> 4. **影响镜像的可重用性** `Docker` 镜像是不可变的，这意味着每次创建新容器时都是从相同的镜像开始。这个特性促进了容器的可重用性和可移植性，但也意味着容器内部的任何状态或变化（例如日志文件、数据库文件）都不会持久保存。
> 5. **应用状态管理** 对于有状态应用，推荐使用外部数据库或持久化存储解决方案。这与 `Docker` 容器本身的短暂性和无状态特性相一致。使用外部存储，可以确保数据在容器重新启动或替换后仍然可用。

## 3.4.k8s 的服务发现



## 3.5.k8s 的存储状态

`DBMS` 和 `LVS`

## 3.6.k8s 的调度管理



## 3.7.k8s 的安全机制



## 3.8.k8s 的软件管理



## 3.9.k8s 的源码探究



# 4.初步使用工具搭建集群

## 4.1.Minilube

### 4.1.1.minilube 的初始

这里我使用 `minilube` 来学习 `k8s`。

```shell
# 使用 minilube 初始化集群
$ sudo usermod -aG docker $USER
$ newgrp docker
$ minikube start
# 实际上 minikube start  这个指令拉取了镜像文件(默认使用 Docker)
# 可以使用 sudo docker image 来查看拉取的镜像文件如下
# gcr.io/k8s-minikube/kicbase   v0.0.45   aeed0e1d4642   4 weeks ago   1.28GB
# gcr.io/k8s-minikube/kicbas 这个基础镜像内部包含了运行 k8s 的基础工具), 适用于 Minikube 的 KIC 功能(Kubernetes In Docker, 一个在 Docker 容器中运行 Kubernetes 的方法), minikube start 还会启动这个镜像为容器, 方便开发者进行集群部署, 您可以使用 sudo docker container ls 进行查看

# minikube stop # 停止集群
# minikube delete # 删除集群
# 可以配置 alias kubectl="minikube kubectl --" 提高终端体验, Minikube 自带一个 kubectl 客户端, 和 Minikube 兼容性较好, 这也是一个最佳实践

```

### 4.1.2.minilube 的操作

#### 4.1.2.1.minilube 的原理

实际上在 `minilube` 中使用的 `k8s` 是装载进一个容器中运行的，而容器内部又下载了镜像文件，运行了多个子容器。因此，如果环境不支持嵌套虚拟化，则没办法运行 `minilube` 工具。

#### 4.1.2.2.minilube 的查看

```shell
# 查看集群
$ kubectl cluster-info # 显示 Kubernetes 集群的基本信息

$ kubectl get pods --all-namespaces # 查看所有的 pod, pod 通常代表 k8s 中的一个最小调度单位(可以由一个或多个容器组成), 可以理解为一个逻辑主机, 拥有唯一的 ip 地址, 共享网络端口资源, 共享存储卷资源, 简化指令为 kubectl get pod -A
NAMESPACE     NAME                               READY   STATUS    RESTARTS      AGE
kube-system   coredns-6f6b679f8f-lsczv           1/1     Running   0             76m
kube-system   coredns-6f6b679f8f-x62pf           1/1     Running   0             76m
kube-system   etcd-minikube                      1/1     Running   0             76m
kube-system   kube-apiserver-minikube            1/1     Running   0             76m
kube-system   kube-controller-manager-minikube   1/1     Running   0             76m
kube-system   kube-proxy-82hjt                   1/1     Running   0             76m
kube-system   kube-scheduler-minikube            1/1     Running   0             76m
kube-system   storage-provisioner                1/1     Running   2 (76m ago)   76m
# 这些 pod 其实就是 gcr.io/k8s-minikube/kicbase 镜像运行为容器后中的子容器
```

#### 4.1.2.3.minilube 的启停

```shell
# 启停容器
$ minikube pause # 暂停集群
$ minikube unpause # 启动集群
$ minikube stop # 停止集群(就相当于关闭容器的运行)

```

#### 4.1.2.4.minilube 的插件

```shell
# 使用插件
$ minikube addons list # 浏览插件
$ minikube addons enable dashboard # 打开其中一个插件
$ minikube dashboard # 启动插件
# 这将在本地部署一个面板网站, 在浏览器中访问返回的地址即可打开该面板
```

![260f61c222ca5e1d800fc11204f88e43](./assets/260f61c222ca5e1d800fc11204f88e43-1728388537284-7.png)

>   补充：解决代理问题，我这里刚好有三个镜像是拉取失败的。
>
>   ```shell
>   # 解决远端镜像问题
>   # 进入 minikube ssh 终端
>   $ minikube ssh
>   
>   # 在一些国内存储库中(例如阿里云)拉取镜像文件
>   $ docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/metrics-server:v0.7.2
>   $ docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/metrics-server:v0.7.2 kubernetes/metrics-server:v0.7.2
>   
>   $ docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/dashboard:v2.7.0
>   $ docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/dashboard:v2.7.0 kubernetesui/dashboard:v2.7.0
>   
>   
>   # 修改镜像拉取策略, 优先使用本地镜像而不是拉取远端镜像
>   $ kubectl edit deployment kubernetes-dashboard -n kubernetes-dashboard
>   $ kubectl edit deployment dashboard-metrics-scraper -n kubernetes-dashboard
>   $ kubectl edit deployment metrics-server -n kube-system
>   # 如下图红框处进行修改
>   
>   ```
>
>   ![image-20241008194935021](./assets/image-20241008194935021.png)
>
>   ```shell
>   # 退出 检查此时的状态是否都为 runing
>   $ exit
>   
>   $ kubectl get pod -A
>   NAMESPACE              NAME                                        READY   STATUS    RESTARTS       AGE
>   kube-system            coredns-6f6b679f8f-lsczv                    1/1     Running   11 (62m ago)   6h36m
>   kube-system            coredns-6f6b679f8f-x62pf                    1/1     Running   11 (62m ago)   6h36m
>   kube-system            etcd-minikube                               1/1     Running   11 (63m ago)   6h36m
>   kube-system            kube-apiserver-minikube                     1/1     Running   11 (62m ago)   6h36m
>   kube-system            kube-controller-manager-minikube            1/1     Running   12 (63m ago)   6h36m
>   kube-system            kube-proxy-82hjt                            1/1     Running   11 (63m ago)   6h36m
>   kube-system            kube-scheduler-minikube                     1/1     Running   11 (63m ago)   6h36m
>   kube-system            metrics-server-667d84658b-k82fh             1/1     Running   0              98s
>   kube-system            storage-provisioner                         1/1     Running   20 (61m ago)   6h36m
>   kubernetes-dashboard   dashboard-metrics-scraper-6fb8867c9-cw57k   1/1     Running   0              8m17s
>   kubernetes-dashboard   kubernetes-dashboard-6b7bbd4ddc-tx7vt       1/1     Running   0              30m
>   
>   $ minikube dashboard # 非常完美运行起来...
>   🤔  正在验证 dashboard 运行情况 ...
>   🚀  正在启动代理...
>   🤔  正在验证 proxy 运行状况 ...
>   🎉  正在使用默认浏览器打开 http://127.0.0.1:33045/api/v1/namespaces/kubernetes-dashboard/services/http:kubernetes-dashboard:/proxy/ ...
>   Failed to open display
>   Warning: failed to launch javaldx - java may not function correctly
>   
>   ```
>
>   其他情况下也都可以这么进行配置...

## 4.2.Kind

