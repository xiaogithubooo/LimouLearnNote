<style>
.heimu {
    position: relative; /* 使伪元素相对于其父元素定位 */
    display: inline-block; /* 使元素内容包裹 */
    color: transparent; /* 隐藏文字颜色 */
    text-decoration: none; /* 去掉链接下划线 */
}
.heimu::before {
    content: ''; /* 生成一个空内容伪元素 */
    position: absolute; /* 绝对定位 */
    top: 0;
    left: 0;
    width: 100%;
    height: 100%;
    background-color: #F5F5F5; /* 白色遮盖 */
    z-index: 1; /* 确保覆盖文字 */
}
.heimu:hover::before {
    background-color: transparent; /* 鼠标悬浮时移除黑色遮盖 */
}
.heimu:hover {
    color: white !important; /* 鼠标悬浮时显示白色文字 */
    text-shadow: none; /* 去掉文本阴影 */
}
</style>

**如果本系列文章对您有帮助，可以 [star 一下我的 limou-learn-note](https://github.com/xiaogithubooo/LimouLearnNote)，求求惹(๑＞ ＜)☆♡~**

**叠甲：以下文章主要是依靠我的实际编码学习中总结出来的经验之谈，求逻辑自洽，不能百分百保证正确，有错误、未定义、不合适的内容请尽情指出！**

[TOC]

>   概要：

>   资料：

---

# 1.Docker 的历史

容器技术的快速发展改变了现代软件开发和部署的方式，但是这种技术并不新颖，从很久开始就发展起来了。

## 1.1.监狱时期

1.   早期的贝尔实验室发明了 `chroot`，主要是为了避免在一个系统软件编译和安装完成后，需要重写配置环境的情况。因此就考虑能否创建出一个隔离环境，专门用于构建和搭建测试环境，进而得到了一个把进行文件系统隔离的早期容器技术。

     `chroot` 系统调用可以将进程和子进程的根目录更改为文件系统中的新位置（而不是系统的根目录），隔离之后该进程就无法访问外面的文件，如同一个监狱一样（`Chroot jail`），后续只需要把测试信息放在 `Jail` 中就可以完成测试了。

     ```cpp
     // 演示关于 chroot() 的使用
     #include <unistd.h>
     #include <stdio.h>
     #include <stdlib.h>
     
     int main() {
         const char *new_root = "/path/to/new/root";
     
         // 切换根目录
         if (chroot(new_root) != 0) {
             perror("chroot failed");
             return 1;
         }
     
         // 切换当前工作目录到新根目录
         if (chdir("/") != 0) {
             perror("chdir failed");
             return 1;
         }
     
         // 在新的根目录下执行命令
         printf("Now in chrooted environment\n");
     
         // 例如执行一个命令，这里使用 system 函数
         system("/bin/sh");
     
         return 0;
     }
     
     ```

2.   到了后来的 `FreeBSD Jail` 让每一个 `Jail` 成为在主机上运行的虚拟环境，有自己的文件管理、进程管理、用户管理，甚至还可以为每个虚拟环境分配一个 `ip` 地址，使得网络空间也被隔离开来，本质也是一种监狱机制。

3.   后面还退出了一种叫 `Linux VServer` 的基于内核的虚拟化技术，可以在单个 `Linux` 内核实例上创建多个独立的虚拟环境（称为 **安全上下文** 或 **虚拟服务器**）。这些虚拟环境共享相同的内核，但它们在用户空间中是隔离的，每个环境可以有自己的文件系统、网络接口和进程空间。如果您想尝试这种技术，可以安装 `VServer` 试一试，这里举一个 `Ubuntu` 的大概伪例子。

     (1)首先您需要一个支持 `VServer` 的内核以及相关的用户空间工具,这通常需要从源代码编译或从特定的包管理器安装，也就是 `sudo apt-get install linux-image-vserver vserver-debiantools`

     (2)然后创建新虚拟服务器,使用 `vserver` 工具来创建和管理虚拟服务器。创建一个新的虚拟服务器实例，并指定其根文件系统 `sudo vserver <vserver_name> build...`

     (3)使用 `sudo vserver <vserver_name> start` 启动虚拟服务器

     (4)使用 `sudo vserver <vserver_name> enter` 命令进入虚拟服务器的上下文环境

     (5)管理虚拟服务器...

4.   `Solaris Containers` 软件公测，结合系统资源控制和区域进行了隔离，增加了快照和克隆功能，但是没有太大的发展（也没有那么强需求的场景），依旧是以 `Jail` 模式为核心

## 1.2.云计算时期

云时代是 `Google` 率先提出的概念，进而发生了各种云厂商，现代的云已经变成了强需求，变成和水电一样的基础设施。云的存在使得个人或企业无需操心大量数据的处理带来的高资源消耗，用户只需要向云服务厂商花费较低的钱，就可以得到高回报的云资源。云的存在使得隔离技术变成核心的诉求。

`Google` 推出 `Process Containers/Control Groups` 被合并到 `Linux2.6.24` 版本中，旨在限制、统计、隔离一组进程的资源使用（这里的资源包括 `CPU, 内存, 磁盘IO, 网络...`）。

-   此时控制和隔离技术都发展起来了，顺势就发展起了 `LXC` 这个比较完整的 `Linxu` 容器管理技术，其内部使用 `Control Groups`（可以控制一个进程组的资源限制和资源管理）和 `Linux` 命名空间 `namespace` 来实现（处于不同命名空间的进程组在使用资源的时候，只能感受到自己的资源），可以在单个 `Linux` 上运行，无需任何补丁。
-   而后推出的 `GAE(Goolge App Engine)` 则首次把开发平台当作一种服务来实现，采用云计算技术，跨越多个服务器和数据中心来虚拟化引用程序。而 `GAE` 内部使用了 `Borg` 来对容器进行编排和调度

因此早期的 `LXC` 和 `Borg` 其实就相当于最早的 `Docker` 和 `k8s`，前者是容器技术，后者是编排技术。

## 1.3.云原生时期

容器技术比较完善了，但是容器技术只是解决了容器化和分发问题，没有解决但是如何让软件自动部署、自动更新、自动发布、自动回滚、监控、镜像部署等问题，道理云原生时代时，特别强调微服务化、容器化、持续交付。

云原生和传统云计算之间有一些显著的区别，以下是它们的主要对比：

1.   **架构设计** 云原生基于微服务架构，应用被拆分成多个独立的、自治的服务，每个服务负责特定的功能。这种设计使得应用程序更易于扩展、维护和更新；传统云通常依赖于单体架构，即整个应用作为一个整体进行部署和管理。这种架构可能导致单点故障，并且难以进行独立的扩展和维护。
2.   **容器化与虚拟化** 云原生广泛使用容器（如 Docker）来打包和部署应用。容器提供了应用的环境隔离，并且支持高效的资源利用和快速的部署。主要依赖虚拟机来部署应用。虚拟机提供了完整的操作系统虚拟化，虽然隔离性较好，但启动速度慢，资源利用率不如容器高效。
3.   **持续集成与持续交付（CI/CD）** 云原生强调持续集成和持续交付的实践，通过自动化工具和管道来实现快速的代码提交、测试和部署，确保高频率的交付和快速反馈；传统云可能使用较为传统的部署方式，手动干预较多，更新和发布的周期较长，自动化程度较低。
4.   **编排与管理** 云原生使用动态编排工具如 Kubernetes 来自动管理容器的部署、扩展、负载均衡和故障恢复。这种工具能够高效地管理复杂的分布式应用；传统云通常使用手动的虚拟机管理和配置工具，自动化程度低，管理复杂度高。
5.   **开发与运维** 云原生强调开发（Dev）和运维（Ops）的紧密结合（DevOps），通过自动化工具和实践实现高效的开发和运营；传统云开发和运维可能分开管理，缺乏一体化的自动化工具，导致部署和运维过程较为复杂和耗时。
6.   **扩展与弹性** 云原生支持弹性扩展，可以根据需求动态增加或减少资源。微服务和容器化使得应用可以轻松地扩展和缩减；传统云扩展通常依赖于虚拟机的增减，可能需要手动干预，并且扩展和缩减的速度较慢。
7.   **依赖管理** 云原生使用服务发现、负载均衡和配置管理等机制来处理服务间的依赖关系和通信；传统云可能依赖于固定的 IP 地址和手动配置，处理服务间的依赖和通信更加复杂和不灵活。

# 2.Docker 是什么

目前最流行的容器化技术是 `Docker`，最流行的容器管理服务是 `Kubernetes(K8s)`，应用/服务可以打包为 `Docker` 镜像，通过 `K8s` 来动态分发和部署镜像。`Docker` 镜像可理解为一个刚好能运行应用/服务的最小操作系统，内部存放应用/服务的运行代码。

运行环境根据实际的需要提取设置好后，把整个“小操作系统”打包为一个镜像后，就可以分发到需要部署相关服务的机器上，直接启动 `Docker` 镜像就可以把服务运行起来，使服务的部署和运维变得非常简单。

`Docker` 不仅仅是解决了容器化问题，还直接给云厂商提供了解决方案。

# 3.Docker 的下载

下面演示在 `Ubuntu20.04.4 LTS` 下关于 `Docker` 换阿里源的安装，<span class="heimu" title="你知道的太多了">呵，那堵墙就是个 sb 玩意</span>

```sh
# Ubuntu20.04.4 LTS 安装 Docker
# 更新包管理器并且安装必要的依赖包
$ sudo apt-get update && sudo apt-get install \
    apt-transport-https \
    ca-certificates \
    curl \
    gnupg \
    lsb-release

# 添加 Docker 的 GPG 密钥
$ curl -fsSL https://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg
# 如果您可以使用官方源: curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg

# 添加 Docker 软件源
echo \
"deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://mirrors.aliyun.com/docker-ce/linux/ubuntu \
$(lsb_release -cs) stable" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
# 如果您可以使用官方源:
# echo \
# "deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu \
# $(lsb_release -cs) stable" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null

# 更新包管理器并且开始安装 Docker
sudo apt-get update && sudo apt-get install docker-ce docker-ce-cli containerd.io docker-compose-plugin -y

# 验证 Docker 安装版本
docker --version
Docker version 27.1.1, build 6312585
```

# 4.Docker 的基础

## 4.1.虚拟和容器的原理

-   物理机：真实的计算机
-   虚拟化：从物理机中抽象出来的多台逻辑计算机/虚拟机，每台虚拟机都可以运行不同的操作系统，这些操作系统运行在宿主机器的原生操作系统上，依赖于原生操作系统
-   容器化：又叫“操作系统层虚拟化”，可以将操作系统内核抽象为多个逻辑内核（虚拟内核和虚拟内核之前有可能是不一样的），对于每个实例的拥有者来说，有点像进程地址空间一样，会误以为内核专属于自己，而 `docker` 目前就是容器技术的事实标准（容器技术需要两个基础，一个是通过命名空间控制资源视图，一个是通过控制组限制运行资源）

![容器vs虚拟机](./assets/TB1xeudKrvpK1RjSZFqXXcXUVXa-2336-907.png_.png)

挑重点来说的话就是：

-   虚拟机伪造的是硬件物理接口，进而实现在一个操作系统上加载别的操作系统的源代码
-   而容器伪造的是操作系统接口，进而实现在一个操作系统上模拟其他操作系统系统调用

>   补充：我们前面提到的虚拟机的实现虚拟机监视器作为宿主机器的软件被调度，而其实也有直接运行在物理机硬件上的虚拟机监视器软件，我们之前使用的 `Linxu` 虚拟机软件 `Vmware Workstation` 就属于前者，而 `Vmware ESX` 属于后者。

而一旦提及到容器化或 `docker`，就不得不提及两种非常重要的技术，一个是命名空间技术，一个是控制组技术。

-   **命名空间技术** 您可以了解下 `unshare` 指令或接口的使用，这个指令您会使用就明白什么是命名空间了

-   **控制组技术** 则需要通过 `mount` 指令来实现，不过您需要先使用 `cat /proc/filesystems | grep cgroup` 查看您的系统是否支持 `cgroup`，如果是较老版本可能没有显示，或者显示 `nodev cgroup`，较新的版本还具有 `nodev cgroup2`。并且可以使用 `cat /proc/cgroups` 查看可以控制的资源列表（这里面支持的每一种资源也可以叫子系统或者资源控制组，每个子系统代表了一种特定的资源管理功能）。

    ```shell
    # 检查 cgroups 的版本
    $ cat /proc/cgroups
    #subsys_name    hierarchy       num_cgroups     enabled
    cpuset  0       118     1
    cpu     0       118     1
    cpuacct 0       118     1
    blkio   0       118     1
    memory  0       118     1
    devices 0       118     1
    freezer 0       118     1
    net_cls 0       118     1
    perf_event      0       118     1
    net_prio        0       118     1
    hugetlb 0       118     1
    pids    0       118     1
    rdma    0       118     1
    misc    0       118     1
    ```
    
    我们把应用了某些资源限制的一组进程的集合称为一个控制组，`cgroup` 的资源控制非常有趣，是分层设计的，每个层级可以包含多个子系统，每个子系统可以应用不同的资源限制。
    
    那怎么使用控制组呢？首先您需要明白 `Linux` 提供的控制组 `cgroup/cgroup2` 其实是一种基于文件系统接口的机制，所以也应该当作一种文件系统来对待，既然是文件系统，就可以把对应的文件系统的设备挂载起来，操作系统一般默认挂载到操作系统下的 `/sys/fs/cgroup/` 目录下，不过有可能有的操作系统没有这个行为，您可以自己手动进行挂载。
    
    ```shell
    # 检查是否有默认的挂载目录
    $ mount | grep cgroup # 查看是否有挂载点
    cgroup2 on /sys/fs/cgroup type cgroup2 (rw,nosuid,nodev,noexec,relatime,nsdelegate,memory_recursiveprot)
    
    $ ls /sys/fs/cgroup # 检查该目录的存在
    aegis                   cgroup.threads         io.cost.qos                    sys-fs-fuse-connections.mount
    aegisMonitor            cpu.pressure           io.pressure                    sys-kernel-config.mount
    aegisPythonLoder0       cpuset.cpus.effective  io.prio.class                  sys-kernel-debug.mount
    cgroup.controllers      cpuset.mems.effective  io.stat                        sys-kernel-tracing.mount
    cgroup.max.depth        cpu.stat               memory.numa_stat               system.slice
    cgroup.max.descendants  dev-hugepages.mount    memory.pressure                user.slice
    cgroup.procs            dev-mqueue.mount       memory.stat
    cgroup.stat             init.scope             misc.capacity
    cgroup.subtree_control  io.cost.model          proc-sys-fs-binfmt_misc.mount
    ```
    
    如果不存在这个目录，那就使用这条指令进行手动挂载，不过有个比较重要的问题是，实际上没有一种设备是属于 `cgroup/cgroup2` 这种设备因此实际上我们设备使用 `none` 作为占位符就行，不用把任何文件或设备进行挂载。
    
    ```shell
    # 手动挂载(如果您的系统已挂载了则无需操作这一步)
    sudo mkdir -p /sys/fs/cgroup/ && sudo mount -t cgroup2 none /sys/fs/cgroup
    ```
    
    然后我们才能开始创建一个控制组，如下进行操作即可。
    
    ```shell
    # 创建控制组并且把某些进程加入控制组中
    # 创建一个控制组并且打开
    sudo mkdir /sys/fs/cgroup/my_group/ && cd /sys/fs/cgroup/my_group/
    
    # 了解在控制组目录下的配置文件(以下文件您可以在通过 tree | grep "..." 的形式查看系统内其他的控制组是怎么设置的, 另外不同版本的 cgroup 有可能文件名后缀不同, 这里演示的是 cgroup2 的)
    	# /sys/fs/cgroup/<controller>/cgroup.procs: 这个配置文件用于列出属于该控制组的所有进程 pid, 您可以将进程 pid 写入到这个文件中来将进程加入到该控制组中, 并且使用换行区分不同的进程 pid
    	# /sys/fs/cgroup/<controller>/memory.max: 设置控制组的最大内存使用限制, 写入 -1 则代表无限制, 若写入其他正整数值 num 则代表不得超过 num byte
    	# /sys/fs/cgroup/<controller>/cpu.max: 设置控制组的 CPU 使用限制, 写入 <max> <period>, 其中 <max> 表示允许的最大 CPU 时间(以毫秒为单位), <period> 表示计算 CPU 使用时间的周期(以毫秒为单位), 整体代表每 period 周期内运行的 max 的 CPU 时间。特殊的, 如果写入 max 1000000 则代表没有限制
    	# /sys/fs/cgroup/<controller>/pids.max: 设置控制组中可以创建的最大进程数, 直接写入一个正整数值即可。特殊的, 如果写入 max 则代表没有限制
    	# /sys/fs/cgroup/<controller>/io.max: 设置 IO 设备的限制，例如读写速率限制, 写入 <READ_RATE> <WRITE_RATE>, 分别指读和写的速率限制(以字节每秒为单位)。特殊的, max max 代表对读写毫无限制
    	# /sys/fs/cgroup/<controller>/cgroup.subtree_control: 在 cgroup 中控制启用或禁用某个子系统, 也就是随时启用和禁用某种资源限制, 书写格式例如 +memory +cpu 代表启用 memory 和 cpu 子系统, -memory 代表禁用 memory 子系统
    	# ...
    
    # 不过我们不直接写入配置文件, 我们使用一个面板程序来测试不同文件的配置是否能够生效
    cat test_cgroup.cpp
    
    ```

>   吐槽：有时会看到把两者都当作虚拟技术的情况，这种应该是认为两者实际上都是伪装术，只不过伪装的颗粒度不同的，因此统称为虚拟技术确实是可以的。另外还有一种模拟函数库的虚拟技术，最有名的就是 `Java` 的 `JVM` 虚拟机，模拟的就是函数库的接口，也就可以做到所谓的“一处运行，到处跑”。

## 4.2.虚拟和容器的作用

为什么需要虚拟技术或容器技术呢？它们都主要是为了方便以下一些场景：

-   资源细分化利用率高
-   环境标准化保证运行、开发、测试环境一致
-   资源弹性伸缩
-   差异化环境，可以使得不同项目满足不同的系统环境
-   避开不安全软件对整个物理机的影响，也就是沙箱安全
-   比虚拟机更高效，因为是对内核的抽象，启动更快直接运行在宿主机器上，启动更快

## 4.3.启动和关闭 Docker



---

>   结语：...

**如果本系列文章对您有帮助，可以 [star 一下我的 limou-learn-note](https://github.com/xiaogithubooo/LimouLearnNote)，求求惹(๑＞ ＜)☆♡~**

