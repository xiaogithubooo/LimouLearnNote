

# 1.AI绘画从零开始的AI绘画入门教程

所用到的全部软件皆为开源软件：stable diffusion webui。

# 2.AI做不到的

目前的AI无法做到给一张图片就直接会照着画出来无数张这个物体或人物的图片的。

就比如说：有一个立绘，想让AI只根据这一个立绘就完美画出来各种动作角度，抱歉，做不到或者说是技术限制，目前根本就做不好（这有个专业术语叫`one shot`，即“一次尝试”）。想法是美好的，现实是残酷的。目前局限性很大。

# 3.AI做得到的

从二次元到真人，只要是能画出来的都能画。（秋不赞同画真人，本篇教程也完全不会涉及到任何相关内容。）

1. 通过文字描述凭空生成一张图片（文生图功能）

2. 在基础图片上重新绘制或修改一幅图片（图生图功能）

尽管无法绘画出各种角度，但是AI经过一定调教可以做到画风的模拟（训练模型），在拥有足够的素材进行训练以后，就可以学习到一个人物或者画师的画风。通常来说，要拥有较好的效果，人物学习最低需要10张以上。画风学习需要50张以上。

# 4.AI绘画所需配置

在当前启动器下所推荐配置：拥有Nvidia独立显卡、RTX20系以后的显卡。仅生成图片推荐8G显存（4G是最低保障配置）训练推荐大于12G（越大越好）内存推荐16G及以上。硬盘推荐使用固态硬盘，否则你开软件要等个5-10分钟。CPU不做太多要求。

A卡能用，但是性能损耗很大。可以在Linux系统上获得最佳效果。

# 5.下载安装整合包

秋于2023/4/16更新最新的整合包，无需任何替换即可达到最佳速度，打开即用，内置启动器，详情见[视频链接](https://www.bilibili.com/video/BV1iM4y1y7oA/)。

# 6.下载时遇到问题

启动器内拥有 “疑难解答” 功能，可以自动扫描绝大部分问题。同时秋也写了[答疑专栏](https://www.bilibili.com/read/cv20514282?from=articleDetail)，会定期更新一些常见问题.

# 7.入门基础

![png@1256w_630h_!web-article-pic](https://i0.hdslb.com/bfs/article/watermark/58e248f667f8932526774d1835ccf8f3f25d7009.png@1256w_630h_!web-article-pic.webp)

## 7.1.文生图基础使用

由文字描述直接生成图片（待更新Hiresfix讲解）。

### 7.1.1.正面和反面Tag

通用正面 Tag（想要的内容）改善画质用的 Tag：masterpiece, best quality（杰作，最佳品质）。

通用反面 Tag（不想要的内容），保底不出古神用的 Tag：lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry（低分辨率，糟糕的解剖结构，糟糕的手，文本，错误，缺失的手指，多余的数字，更少的数字，裁剪，最差的质量，低质量，普通质量，jpeg压缩伪影，签名，水印，用户名，模糊）。

整合包内一般都会带一个自动补全Tag的插件，如果不知道那些Tag好，可以使用[标签超市](https://tags.novelai.dev/)

> 另外，你可能会看到别人发的Tag里面会有一些符号？比如大小括号等等。这属于进阶用法，这里仅仅简单提及一下。以girl这个 Tag 作为例子。
> 
> (girl)加权重，这里是1.1倍。括号是可以叠加的，如:（(girl))加很多权重。1.1*1.1=1.21倍
> 
> [girl]减权重，一般用的少。减权重也一般就用下面的指定倍数。
> 
> (girl:1.5)指定倍数，这里是1.5倍的权重。还可以(girl:0.9)达到减权重的效果

### 7.1.2.采样迭代步数和采样器

采样步数不需要太大，一般在50以内。通常28是一个不错的值。

采样器没有优劣之分，但是他们速度不同。全看个人喜好。推荐的是图中圈出来的几个，速度效果都不错。

> 这些是一些采样方法的名称。以下是对每个方法的简要说明：
> 
> 1. Euler: 埃尔方法是一种常用的数值积分方法，用于求解常微分方程。
> 
> 2. LMS: 最小均方算法（Least Mean Squares），主要用于自适应滤波和信号处理中。
> 
> 3. Heun: Heun方法是一种改进的欧拉方法，也用于求解常微分方程。
> 
> 4. DPM2: 二阶偏微分方法（Second-Order Partial Differential Method），用于图像处理和计算机视觉中。
> 
> 5. DPM2 a: 与DPM2类似，但经过了改进的版本。
> 
> 6. DPM++ 2S a: DPM++（Double Partial Method）的改进版，用于解决图像处理问题。
> 
> 7. DPM++ 2M: DPM++的另一种改进版本。
> 
> 8. DPM++ SDE: DPM++的随机微分方程版本，用于建模随机过程和噪声。
> 
> 9. DPM fast: DPM++的快速实现版本。
> 
> 10. DPM adaptive: DPM++的自适应方法，根据问题的特性动态地选择合适的计算策略。
> 
> 11. LMS Karras: Karras提出的LMS算法改进版本。
> 
> 12. DPM2 Karras: Karras提出的DPM2算法改进版本。
> 
> 13. DPM2 a Karras: Karras提出的DPM2 a算法改进版本。
> 
> 14. DPM++ 2S a Karras: Karras提出的DPM++ 2S a算法改进版本。
> 
> 15. DPM++ 2M Karras: Karras提出的DPM++ 2M算法改进版本。
> 
> 16. DPM++ SDE Karras: Karras提出的DPM++ SDE算法改进版本。
> 
> 17. DDIM: 延迟差分法（Delayed Difference Method），用于非线性动态系统建模和控制。
> 
> 18. PLMS: 快速最小均方算法（Fast Least Mean Squares），用于自适应滤波和信号处理。
> 
> 19. UniPC: 均匀相位采样（Uniform Phase Sampling），用于图像和信号处理中。
> 
> 这些方法都是在不同领域和问题背景下使用的数值计算或采样方法，具体使用取决于实际需求。
> 
> 这些有能力了再去了解吧……

### 7.1.3.提示词相关代表

![png@1256w_212h_!web-article-pic](https://i0.hdslb.com/bfs/article/df02abbeca054c3d7776c62b35fddd117f85dfea.png@1256w_212h_!web-article-pic.webp)

提示词相关性代表你输入的 Tag 对画面的引导程度有多大，可以理解为 “越小AI越自由发挥”

太大会出现锐化、线条变粗的效果。太小AI就自由发挥了，不看Tag

### 7.1.4.随机种子

随机种子是生成过程中所有随机性的源头，每个种子都是一幅不一样的画。默认的 -1 是代表每次都换一个随机种子。由随机种子，生成了随机的噪声图，再交给AI进行画出来。

## 7.2.图生图基础使用

略

## 7.3.为生成的图片进行放大

AI 基本上无法生成超级大图，想要生成高清图片正确的做法是生成2k以内分辨率的图片再使用放大功能（也叫超分辨率）进行放大图片。

## 7.4.查询图片参数

AI生成图片会自动保存全部参数到原图中，可以在WebUI的 “图片信息” 一栏内通过解析原图查看到。也可以使用秋写的这个[工具](https://spell.novelai.dev/)

![eadc2efb-6ac0-4674-b164-e67680ca132c](file:///C:/Users/Limou_p350ml9/Pictures/Typedown/eadc2efb-6ac0-4674-b164-e67680ca132c.png)

![935f6577-8594-4e81-97e5-28f0f18c1d97](file:///C:/Users/Limou_p350ml9/Pictures/Typedown/935f6577-8594-4e81-97e5-28f0f18c1d97.png)

非AI生成图片或经过压缩的图片可以通过Deepdanbooru、Tagger来尝试反推tag。

![png@1256w_612h_!web-article-pic](https://i0.hdslb.com/bfs/article/ebf0c083097b726f5bc986e135794bab626c4de0.png@1256w_612h_!web-article-pic.webp)

Deepdanbooru 默认已经自带了，可以在图生图页面找到。

Tagger是一个插件，在新版的整合包内也帮你装好了，可以在顶栏找到。如果没有的话需要自行安装，安装教程参考下面的插件管理部分，[视频较早，仅供参考](https://www.bilibili.com/video/BV17e4y1m7GZ/)。

![png@1256w_638h_!web-article-pic](https://i0.hdslb.com/bfs/article/487b0328473d86c8c3ddd2b5055dd586d58abd60.png@1256w_638h_!web-article-pic.webp)

## 7.5.使用X/Y图表

在生成图片时候，可以使用x/y图表快速生成不同参数的图片进行对比。

一个图表示例，常用做对比图
（待更新专栏，可以先看看下面这个网站）

https://guide.novelai.dev/guide/configuration/param-advanced#x-y-z-图表

## 7.6.使用图库浏览器

使用图库浏览器插件管理生成的图片。

## 7.7安装/管理/更新插件

通过 WebUI 自带的插件管理系统安装插件，并且及时更新。推荐使用启动器进行更新插件，更为快捷。

# 8.进阶使用

## 8.1.使用高级提示词语法

> 提示词是提示而不是判定依据

### 8.1.1.正向标签

[提示词权重、分步渲染相关内容](https://guide.novelai.dev/guide/prompt-engineering/txt2img#提示词语法)

1. 单词标签：对于在标签单词上特化训练的模型，建议使用逗号隔开的单词作为提示词。
   普通常见的单词，例如是可以在数据集来源站点找到的著名标签（比如 Danbooru)。单词的风格要和图像的整体风格搭配，否则会出现混杂的风格或噪点。
   避免出现拼写错误。NLP 模型可能将拼写错误的单词拆分为字母处理。

2. 自然语言：对于在自然语言上特化训练的模型，建议使用描述物体的句子作为提示词。
   取决于训练时使用的数据集，可以使用英文，日文，特殊符号或一些中文。大多数情况下英文较为有效。
   避免 `with` 之类的连接词或复杂的语法，大多数情况下 NLP 模型只会进行最简单的处理。

3. Emoji：Emoji (💰,💶,💷,💴,💵,🎊,🪅🪄,🎀,👩‍🚀) 表情符号也是可以使用并且 **非常准确** 的。
   Emoji 因为只有一个字符，所以在语义准确度上表现良好。
   Emoji 在构图上有影响，比如 `💐☺️💐`。
   [Emoji List, v15.0 (unicode.org)表情参考](https://unicode.org/emoji/charts/emoji-list.html)

4. 颜文字：对于使用 Danbooru 数据的模型来说，可以使用颜文字在一定程度上控制出图的表情。
   例如：`:-)` 微笑 `:-(` 不悦 `;-)` 使眼色 `:-D` 开心 `:-P` 吐舌头 `:-C` 很悲伤 `:-O` 惊讶 张大口 `:-/` 怀疑
   仅支持西方[颜文字](https://danbooru.donmai.us/wiki_pages/tag_group%3Aface_tags)。

5. 空格：逗号前后的少量空格并不影响实际效果。开头和结尾的额外空格会被直接丢弃。词与词之间的额外空格也会被丢弃。

6. 标点符号：用逗号、句号、甚至是空字符（`\0`）来分隔关键词，可以提高图像质量。目前还不清楚哪种类型的标点符号或哪种组合效果最好。当有疑问时，只要以一种使提示更容易被阅读的方式来做。

7. 艺术风格词：可以通过指定风格关键词来创作带有特效或指定画风的图片。参考资料：[NovelAI 使用教程](https://space.bilibili.com/8612008/channel/collectiondetail?sid=787691)和[魔咒课堂人偶教室的测试记录](https://www.yuque.com/longyuye/lmgcwy)[风格化: 32 种](https://www.bilibili.com/video/BV1TP411N71t/)。

8. 运动与姿势：如果没有很大要求的话，选择只与少数姿势相关的提示。
   这里的姿势是指某一事物的物理配置：图像主体相对于摄像机的位置和旋转，人类/机器人关节的角度，果冻块被压缩的方式，等等。你试图指定的事物中的差异越小，模型就越容易学习。
   因为运动就其定义而言涉及到主体姿势的巨大变化，与运动相关的提示经常导致身体的扭曲，如重复的四肢。另外，因为人类的四肢，特别是人类的手和脚有很多关节，他们可以采取许多不同的、复杂的姿势。这使得他们的可视化特别难学，对于人类和神经网络都是如此。
   简而言之：人类站着/坐着的好形象很容易，人类跳着/跑着的好形象很难。

#### 8.1.1.1.模板

先想一下要画什么，例如：主题，外表，情绪，衣服，姿势，背景 一类，然后参考数据集标签表（如果有的话，比如`Danbooru`,`Pixiv`等）。

然后将想要的相似的提示词组合在一起，请使用英文半角 `,` 做分隔符，并将这些按从“最重要到最不重要的顺序”排列。

一种模板示例如下：

```
(quality), (subject)(style), (action/scene), (artist), (filters)
```

1. `(quality)` 代表画面的品质，比如 `low res` 结合 `sticker` 使用来 “利用” 更多数据集， `1girl` 结合 `high quality` 使用来获得高质量图像。
2. `(subject)` 代表画面的主题，锚定画面内容，这是任何提示的基本组成部分。
3. `(style)` 是画面风格，可选。
4. `(action/scene)` 代表动作/场景，描述了主体在哪里做了什么。
5. `(artist)` 代表艺术家名字或者出品公司名字。
6. `(filters)` 代表一些细节，补充。可以使用 艺术家，工作室，摄影术语，角色名字，风格，特效等等。

#### 8.1.1.2.大小写

CLIP 的标记器在标记之前将所有单词转为小写。其他模型，如 BERT 和 T5，将大写的单词与非大写的单词区别对待。

但避免涉及特殊语法，以防被解释为其他语义，例如 `AND`。

#### 8.1.1.3.词汇顺序

[demystifying_prompting_what_you_need_to_know/](https://www.reddit.com/r/StableDiffusion/comments/yjwuls/demystifying_prompting_what_you_need_to_know/)

似乎 VAE 使用了一种称为贝叶斯定理的统计方法。在计算标记的去向时，前几个单词似乎锚定了其余单词标记在潜在空间中的分布。

早期的标记具有更一致的位置，因此神经网络更容易预测它们的相关性。在贝叶斯推理中，矩阵中的第一个标记或证据很重要，因为它设置了初始概率条件。但是后面的元素只是修改了概率条件。因此，至少在理论上，最后的令牌不应该比前面的令牌具有更大的影响。

但是解析器理解事物的方式是不透明的，因此没有办法**确切**地知道词法顺序是否具有“锚”效应。

可以 [使用 X/Y 图自动生成各种顺序](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/1607) 进行尝试。

#### 8.1.1.4.提示词长度

**避免过长的提示词**，提示词放入的顺序就是优先级。由于提示词的权重值从前向后递减，放置在特别靠后的提示词已经对图片的实际生成影响甚微。

不堆叠提示词是一个好习惯，但是如果你确实有很多内容要写，可以适当提高生成步数，以便在生成过程中更好地利用提示词。

SD-WebUI 突破最多 75 个词组限制的方式是将每 20 + 55 个词分为一组。选项 `Increase coherency by padding from the last comma within n tokens when using more than 75 tokens` 让程序试图通过查找最后 N 个标记中是否有最后一个逗号来缓解这种情况，如果有，则将所有经过该逗号的内容一起移动到下一个集合中。该策略可适当缓解提示词过多无法处理的问题，但可能破坏提示词之间的权重关系。

除了 WebUI 对此情况进行了特殊处理外，由于 GPT-3 模型限制，提示词处理空间并不是无限的，大多在在 75-80 之间，75 字符后的内容会被截断。

#### 8.1.1.5.语义失衡

每一个提示词就像染料一样，它们的 “亲和性“ 不同，如果更常见的提示词，比如 `loli` (和其他提示词并列放置)的影响就大于其他提示词。

比如，如果你想生成动漫图片，使用了 星空 `startrail` 标签，相比你期望出现的动漫星空，会有更多来自真实照片的星空元素。

许多词汇在基准上的权重就不一样，所以要根据效果进行合理调节。

### 8.1.2.反向标签

### 8.1.3.权重系数

### 8.1.4.标签系数

### 8.1.5.标签轮转

### 8.1.6.多种提示词生成

## 8.2.更换模型来达到不同的画风/认识不同人物、物体

参考以下文章。

目前可以见到的模型有：大模型、Embedding模型、Hypernet模型、LoRA模型

常用：大模型、LoRA模型。

模型推荐



## 8.3.用模型合并来混合风格

视频内有一部分提到这个。可以参考

（待更新：mbw等新型合并）

## 8.4.使用ControlNet精细控制画面

Controlnet 1.1 版本更新



# 9.带师级别使用

## 9.1.自己进行训练模型

Embedding (Textual inversion)

Hypernetwork

不更新了

LoRA

在今年的5月份，我制作了LoRA训练的界面+一键包。推荐使用这个新视频下载的一键包，打开就可以训练。训练教程仍然参考下方的旧教程，但是不再需要跟着安装了，参数也是用界面调整的



Dreambooth / Native Train （finetune）



## 9.2.训练模型理论基础（涉及部分专业知识）



## 9.3.更加深入理解原理



# 10.前沿探索

都前沿了还需要我引路？自己看去吧。学无止境我也自己在学习。

简单说一下：arXiv翻论文，Github翻最新代码实现。关注关注CVPR等

关注关注沐神（搜李沐）

关注我的最新动态，随时会发点新玩意



# 11.相关推荐

## 11.1.AI绘画相关

@大江户战士 

@只剩一瓶辣椒酱 

@独立研究员-星空 

## 11.2.LoRA训练

@青龙圣者 

@WSH032 

[NovelAI.dev](https://novelai.dev/)

[详细教程/使用文档（部分内容过时）](https://guide.novelai.dev/)

[Huggingface模型站](https://huggingface.co/)

## 11.3.理论AI知识相关

@跟李沐学AI 秋说是唯一神
